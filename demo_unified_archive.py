#!/usr/bin/env python3
"""
Unified Archive System Demo
Shows how to set up and use the PostgreSQL unified archive with real Node Archive Browser data
"""

import os
import sys
import asyncio
import logging
from pathlib import Path

# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)

def print_header(title):
    """Print a fancy header"""
    print("\n" + "="*60)
    print(f"üéØ {title}")
    print("="*60)

def print_step(step_num, description):
    """Print a step with nice formatting"""
    print(f"\nüìç Step {step_num}: {description}")
    print("-" * 50)

def main():
    """Run the unified archive demo"""
    
    print_header("UNIFIED ARCHIVE SYSTEM DEMO")
    print("This demo shows how to consolidate Node Archive Browser conversations")
    print("into a single PostgreSQL database with powerful search capabilities.")
    
    # Demo configuration
    NODE_ARCHIVE_PATH = "/Users/tem/nab/exploded_archive_node"
    DATABASE_URL = "postgresql://localhost/humanizer_archive_demo"  # User can modify
    MAX_CONVERSATIONS = 50  # Limit for demo
    
    print(f"\nüîß Demo Configuration:")
    print(f"   ‚Ä¢ Node Archive Path: {NODE_ARCHIVE_PATH}")
    print(f"   ‚Ä¢ Database URL: {DATABASE_URL}")
    print(f"   ‚Ä¢ Max Conversations: {MAX_CONVERSATIONS} (for demo)")
    
    # Check if Node archive exists
    if not Path(NODE_ARCHIVE_PATH).exists():
        print(f"\n‚ùå Error: Node archive path not found: {NODE_ARCHIVE_PATH}")
        print("Please update the NODE_ARCHIVE_PATH variable in this script.")
        return 1
    
    # Count available conversations
    conversation_dirs = list(Path(NODE_ARCHIVE_PATH).glob("*/conversation.json"))
    print(f"\nüìä Found {len(conversation_dirs)} conversations in Node archive")
    
    if len(conversation_dirs) == 0:
        print("‚ùå No conversation.json files found in the archive path")
        return 1
    
    # Show sample conversation info
    print(f"\nüìã Sample conversations found:")
    for i, conv_file in enumerate(conversation_dirs[:5]):
        folder_name = conv_file.parent.name
        print(f"   {i+1}. {folder_name}")
    if len(conversation_dirs) > 5:
        print(f"   ... and {len(conversation_dirs) - 5} more")
    
    print_step(1, "Database Setup")
    print("To set up the unified archive system:")
    print()
    print("1. Create PostgreSQL database:")
    print(f"   createdb humanizer_archive_demo")
    print()
    print("2. Install required extensions (optional):")
    print("   psql humanizer_archive_demo -c 'CREATE EXTENSION IF NOT EXISTS vector;'")
    print("   psql humanizer_archive_demo -c 'CREATE EXTENSION IF NOT EXISTS pg_trgm;'")
    print()
    print("3. Run the setup script:")
    
    setup_command = f"""python setup_unified_archive.py \\
  --database-url "{DATABASE_URL}" \\
  --node-archive-path "{NODE_ARCHIVE_PATH}" \\
  --max-conversations {MAX_CONVERSATIONS}"""
    
    print(f"   {setup_command}")
    
    print_step(2, "What the Setup Does")
    print("The setup script will:")
    print("‚úÖ Create PostgreSQL unified archive schema")
    print("‚úÖ Run Rails migrations for ActiveRecord models")
    print("‚úÖ Import Node Archive Browser conversations")
    print("‚úÖ Start Enhanced Archive API (port 7200)")
    print("‚úÖ Start Rails API (port 3000)")
    print()
    print("Database schema includes:")
    print("‚Ä¢ Full-text search indexes")
    print("‚Ä¢ Conversation threading")
    print("‚Ä¢ Semantic vector storage")
    print("‚Ä¢ Quality scoring")
    print("‚Ä¢ Multi-source support")
    
    print_step(3, "Example API Usage")
    print("Once setup is complete, you can:")
    print()
    
    print("üîç Search across all conversations:")
    print("""curl -X POST http://localhost:7200/search \\
  -H "Content-Type: application/json" \\
  -d '{
    "query": "quantum mechanics",
    "source_types": ["node_conversation"],
    "limit": 10
  }'""")
    print()
    
    print("üìä Get archive statistics:")
    print("curl http://localhost:3000/api/v1/unified_archive/statistics")
    print()
    
    print("üßµ Get conversation thread:")
    print("curl http://localhost:3000/api/v1/unified_archive/123/thread")
    print()
    
    print("üì§ Export data:")
    print('curl "http://localhost:3000/api/v1/unified_archive/export?format=json"')
    
    print_step(4, "Rails ActiveRecord Usage")
    print("In Rails applications:")
    print()
    print("```ruby")
    print("# Search across all archives")
    print('results = ArchivedContent.search_unified("consciousness", {')
    print('  source_types: ["node_conversation"],')
    print('  author: "user",')
    print('  date_from: 1.month.ago')
    print('})')
    print()
    print("# Get conversation threads")
    print("conversation = ArchivedContent.find(123)")
    print("messages = conversation.conversation_thread")
    print()
    print("# Statistics")
    print("stats = ArchivedContent.statistics")
    print("# => {")
    print('#   total_content: 15420,')
    print('#   by_source_type: {"node_conversation" => 8500},')
    print('#   conversations_count: 1200,')
    print('#   average_quality_score: 0.76')
    print("# }")
    print()
    print("# Convert to WriteBook format")
    print("content.to_writebook_section")
    print("```")
    
    print_step(5, "Expected Results")
    print("After importing your Node Archive Browser data, you'll have:")
    print()
    print("üìà Comprehensive Statistics:")
    print(f"   ‚Ä¢ ~{len(conversation_dirs)} conversations imported")
    print(f"   ‚Ä¢ ~{len(conversation_dirs) * 10} individual messages (estimated)")
    print("   ‚Ä¢ Full-text search across all content")
    print("   ‚Ä¢ Conversation threading preserved")
    print()
    print("üîç Powerful Search Capabilities:")
    print("   ‚Ä¢ Search by keywords, author, date range")
    print("   ‚Ä¢ Cross-conversation discovery")
    print("   ‚Ä¢ Semantic similarity (with vectors)")
    print()
    print("üöÄ Integration Ready:")
    print("   ‚Ä¢ WriteBook section generation")
    print("   ‚Ä¢ Discourse platform publishing")
    print("   ‚Ä¢ LPE enhancement pipeline")
    
    print_step(6, "Next Steps")
    print("1. Run the setup command above")
    print("2. Test the APIs with the example curl commands")
    print("3. Import additional archive sources:")
    print("   ‚Ä¢ Social media exports")
    print("   ‚Ä¢ Email archives")
    print("   ‚Ä¢ Chat platform data")
    print()
    print("4. Set up LPE processing:")
    print("   ‚Ä¢ Content quality assessment")
    print("   ‚Ä¢ Attribute extraction")
    print("   ‚Ä¢ Semantic enhancement")
    print()
    print("5. Configure WriteBook integration:")
    print("   ‚Ä¢ Automatic section generation")
    print("   ‚Ä¢ Quality-based filtering")
    print("   ‚Ä¢ Discourse publishing")
    
    print_header("READY TO CONSOLIDATE YOUR ARCHIVES! üöÄ")
    print("Run the setup command when you're ready to begin.")
    print()
    print("üí° Tip: Start with a small max-conversations value (like 10) for testing,")
    print("   then increase once you've verified everything works correctly.")
    
    return 0

if __name__ == "__main__":
    try:
        exit_code = main()
        sys.exit(exit_code)
    except KeyboardInterrupt:
        print("\nüõë Demo interrupted by user")
        sys.exit(1)
    except Exception as e:
        logger.error(f"‚ùå Demo failed: {e}")
        sys.exit(1)