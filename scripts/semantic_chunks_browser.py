#!/usr/bin/env python3
"""
Semantic Chunks Browser
Interactive tool to explore hierarchical semantic chunks and summaries generated by the embedding system
"""

import psycopg2
from psycopg2.extras import RealDictCursor
import argparse
import json
from datetime import datetime
from typing import Dict, List, Any, Optional
import textwrap

class SemanticChunksBrowser:
    """Interactive browser for semantic chunks with hierarchical summaries"""
    
    def __init__(self, database_url: str = "postgresql://tem@localhost/humanizer_archive"):
        self.database_url = database_url
        
        print("üß† Semantic Chunks Browser")
        print("=" * 40)
        
        # Check connection and get overview
        self._check_database_connection()
        self._show_overview()
    
    def _check_database_connection(self):
        """Check database connection and table existence"""
        try:
            with psycopg2.connect(self.database_url, cursor_factory=RealDictCursor) as conn:
                cursor = conn.cursor()
                cursor.execute("SELECT COUNT(*) FROM semantic_chunks")
                count = cursor.fetchone()['count']
                print(f"‚úÖ Connected to database - {count:,} semantic chunks available")
        except Exception as e:
            print(f"‚ùå Database connection error: {e}")
            raise
    
    def _show_overview(self):
        """Show overview of semantic chunks"""
        with psycopg2.connect(self.database_url, cursor_factory=RealDictCursor) as conn:
            cursor = conn.cursor()
            
            # Level distribution
            cursor.execute("""
                SELECT level, 
                       COUNT(*) as count,
                       COUNT(CASE WHEN summary IS NOT NULL AND summary != '' THEN 1 END) as has_summary,
                       COUNT(CASE WHEN embedding IS NOT NULL THEN 1 END) as has_embedding,
                       AVG(word_count) as avg_words
                FROM semantic_chunks 
                GROUP BY level 
                ORDER BY 
                    CASE level 
                        WHEN 'chunk' THEN 1 
                        WHEN 'message' THEN 2 
                        WHEN 'section' THEN 3 
                        WHEN 'conversation' THEN 4 
                        ELSE 5 
                    END
            """)
            
            levels = cursor.fetchall()
            
            print(f"\nüìä Semantic Chunks Overview:")
            print("-" * 50)
            print(f"{'Level':<12} {'Count':<8} {'Summaries':<10} {'Embeddings':<10} {'Avg Words':<10}")
            print("-" * 50)
            
            for level_data in levels:
                level = level_data['level']
                count = level_data['count']
                summaries = level_data['has_summary']
                embeddings = level_data['has_embedding']
                avg_words = level_data['avg_words'] or 0
                
                summary_pct = (summaries / count * 100) if count > 0 else 0
                embedding_pct = (embeddings / count * 100) if count > 0 else 0
                
                print(f"{level:<12} {count:<8,} {summaries:<4,} ({summary_pct:4.1f}%) {embeddings:<4,} ({embedding_pct:4.1f}%) {avg_words:<10.0f}")
    
    def list_conversations_with_chunks(self, limit: int = 20):
        """List conversations that have semantic chunks"""
        print(f"\nüìö Conversations with Semantic Chunks (Top {limit}):")
        print("-" * 60)
        
        with psycopg2.connect(self.database_url, cursor_factory=RealDictCursor) as conn:
            cursor = conn.cursor()
            
            cursor.execute("""
                SELECT 
                    sc.conversation_id,
                    ac.title,
                    COUNT(*) as chunk_count,
                    COUNT(CASE WHEN sc.level = 'chunk' THEN 1 END) as chunks,
                    COUNT(CASE WHEN sc.level = 'message' THEN 1 END) as messages,
                    COUNT(CASE WHEN sc.level = 'section' THEN 1 END) as sections,
                    COUNT(CASE WHEN sc.level = 'conversation' THEN 1 END) as conversations,
                    SUM(sc.word_count) as total_words
                FROM semantic_chunks sc
                LEFT JOIN archived_content ac ON sc.conversation_id = ac.id
                GROUP BY sc.conversation_id, ac.title
                ORDER BY chunk_count DESC
                LIMIT %s
            """, (limit,))
            
            conversations = cursor.fetchall()
            
            for i, conv in enumerate(conversations, 1):
                title = conv['title'] or f"Conversation {conv['conversation_id']}"
                title = title[:40] + "..." if len(title) > 40 else title
                
                print(f"{i:2d}. [{conv['conversation_id']:6d}] {title}")
                print(f"    Chunks: {conv['chunks']:3d} | Messages: {conv['messages']:3d} | Sections: {conv['sections']:3d} | Words: {conv['total_words']:5,}")
                print()
    
    def explore_conversation_chunks(self, conversation_id: int):
        """Explore all chunks for a specific conversation"""
        print(f"\nüîç Exploring Chunks for Conversation {conversation_id}")
        print("=" * 60)
        
        with psycopg2.connect(self.database_url, cursor_factory=RealDictCursor) as conn:
            cursor = conn.cursor()
            
            # Get conversation info
            cursor.execute("""
                SELECT title, word_count, timestamp 
                FROM archived_content 
                WHERE id = %s
            """, (conversation_id,))
            
            conv_info = cursor.fetchone()
            if conv_info:
                print(f"Title: {conv_info['title'] or 'Untitled'}")
                print(f"Original Words: {conv_info['word_count']:,}")
                print(f"Date: {conv_info['timestamp']}")
            
            # Get chunks by level
            cursor.execute("""
                SELECT level, chunk_id, parent_id, content, summary, word_count, 
                       embedding IS NOT NULL as has_embedding,
                       metadata
                FROM semantic_chunks 
                WHERE conversation_id = %s
                ORDER BY 
                    CASE level 
                        WHEN 'conversation' THEN 1 
                        WHEN 'section' THEN 2 
                        WHEN 'message' THEN 3 
                        WHEN 'chunk' THEN 4 
                        ELSE 5 
                    END,
                    chunk_id
            """, (conversation_id,))
            
            chunks = cursor.fetchall()
            
            if not chunks:
                print("‚ùå No chunks found for this conversation")
                return
            
            # Group by level
            chunks_by_level = {}
            for chunk in chunks:
                level = chunk['level']
                if level not in chunks_by_level:
                    chunks_by_level[level] = []
                chunks_by_level[level].append(chunk)
            
            # Display by hierarchy
            for level in ['conversation', 'section', 'message', 'chunk']:
                if level in chunks_by_level:
                    level_chunks = chunks_by_level[level]
                    print(f"\nüìä {level.title()} Level ({len(level_chunks)} items):")
                    print("-" * 40)
                    
                    for i, chunk in enumerate(level_chunks[:5], 1):  # Show first 5
                        self._display_chunk(chunk, level, i)
                    
                    if len(level_chunks) > 5:
                        print(f"    ... and {len(level_chunks) - 5} more {level} chunks")
    
    def _display_chunk(self, chunk: Dict, level: str, index: int):
        """Display a single chunk with formatting"""
        chunk_id = chunk['chunk_id']
        summary = chunk['summary'] or "No summary"
        word_count = chunk['word_count'] or 0
        has_embedding = "‚úÖ" if chunk['has_embedding'] else "‚ùå"
        
        print(f"  {index}. {chunk_id} ({word_count} words) {has_embedding}")
        
        # Show summary with proper wrapping
        if summary and summary != "No summary":
            wrapped_summary = textwrap.fill(summary[:200], width=70, initial_indent="     ", subsequent_indent="     ")
            print(wrapped_summary)
            if len(summary) > 200:
                print("     ...")
        else:
            # Show content preview if no summary
            content = chunk['content'] or ""
            if content:
                preview = content[:150].replace('\n', ' ')
                wrapped_preview = textwrap.fill(f"[Content]: {preview}", width=70, initial_indent="     ", subsequent_indent="     ")
                print(wrapped_preview)
                if len(content) > 150:
                    print("     ...")
        print()
    
    def search_chunks_by_summary(self, search_term: str, limit: int = 20):
        """Search chunks by summary content"""
        print(f"\nüîç Searching chunks for: '{search_term}'")
        print("-" * 50)
        
        with psycopg2.connect(self.database_url, cursor_factory=RealDictCursor) as conn:
            cursor = conn.cursor()
            
            cursor.execute("""
                SELECT sc.chunk_id, sc.level, sc.conversation_id, sc.summary, sc.word_count,
                       sc.embedding IS NOT NULL as has_embedding,
                       ac.title as conversation_title
                FROM semantic_chunks sc
                LEFT JOIN archived_content ac ON sc.conversation_id = ac.id
                WHERE sc.summary ILIKE %s
                ORDER BY sc.word_count DESC
                LIMIT %s
            """, (f"%{search_term}%", limit))
            
            results = cursor.fetchall()
            
            if not results:
                print("‚ùå No chunks found matching search term")
                return
            
            print(f"Found {len(results)} matching chunks:")
            print()
            
            for i, chunk in enumerate(results, 1):
                conv_title = chunk['conversation_title'] or f"Conversation {chunk['conversation_id']}"
                conv_title = conv_title[:30] + "..." if len(conv_title) > 30 else conv_title
                
                print(f"{i:2d}. [{chunk['level']:4}] {chunk['chunk_id']} ({chunk['word_count']} words)")
                print(f"    From: {conv_title}")
                
                summary = chunk['summary'] or "No summary"
                # Highlight search term
                highlighted_summary = summary.replace(search_term, f"**{search_term}**")
                wrapped_summary = textwrap.fill(highlighted_summary[:200], width=70, initial_indent="    ", subsequent_indent="    ")
                print(wrapped_summary)
                if len(summary) > 200:
                    print("    ...")
                print()
    
    def show_chunk_details(self, chunk_id: str):
        """Show detailed information about a specific chunk"""
        print(f"\nüìÑ Chunk Details: {chunk_id}")
        print("=" * 50)
        
        with psycopg2.connect(self.database_url, cursor_factory=RealDictCursor) as conn:
            cursor = conn.cursor()
            
            cursor.execute("""
                SELECT sc.*, ac.title as conversation_title
                FROM semantic_chunks sc
                LEFT JOIN archived_content ac ON sc.conversation_id = ac.id
                WHERE sc.chunk_id = %s
            """, (chunk_id,))
            
            chunk = cursor.fetchone()
            
            if not chunk:
                print("‚ùå Chunk not found")
                return
            
            print(f"Chunk ID: {chunk['chunk_id']}")
            print(f"Level: {chunk['level']}")
            print(f"Conversation: {chunk['conversation_id']} - {chunk['conversation_title'] or 'Untitled'}")
            print(f"Parent ID: {chunk['parent_id'] or 'None'}")
            print(f"Word Count: {chunk['word_count']:,}")
            print(f"Has Embedding: {'‚úÖ' if chunk['embedding'] else '‚ùå'}")
            print(f"Created: {chunk['created_at']}")
            
            # Show children if any
            if chunk['children']:
                print(f"Children: {', '.join(chunk['children'])}")
            
            # Show metadata
            if chunk['metadata']:
                print(f"\nMetadata:")
                metadata = chunk['metadata']
                for key, value in metadata.items():
                    print(f"  {key}: {value}")
            
            # Show summary
            if chunk['summary']:
                print(f"\nüìù Summary:")
                print("-" * 20)
                wrapped_summary = textwrap.fill(chunk['summary'], width=70)
                print(wrapped_summary)
            
            # Show content preview
            if chunk['content']:
                print(f"\nüìÑ Content Preview (first 500 chars):")
                print("-" * 35)
                content_preview = chunk['content'][:500]
                wrapped_content = textwrap.fill(content_preview, width=70)
                print(wrapped_content)
                if len(chunk['content']) > 500:
                    print("...")
    
    def analyze_summary_quality(self, limit: int = 100):
        """Analyze the quality and characteristics of generated summaries"""
        print(f"\nüìä Summary Quality Analysis")
        print("=" * 40)
        
        with psycopg2.connect(self.database_url, cursor_factory=RealDictCursor) as conn:
            cursor = conn.cursor()
            
            # Summary statistics
            cursor.execute("""
                SELECT 
                    level,
                    COUNT(*) as total_chunks,
                    COUNT(CASE WHEN summary IS NOT NULL AND summary != '' THEN 1 END) as has_summary,
                    AVG(CASE WHEN summary IS NOT NULL THEN LENGTH(summary) END) as avg_summary_length,
                    AVG(word_count) as avg_content_length,
                    AVG(CASE WHEN summary IS NOT NULL AND word_count > 0 
                         THEN LENGTH(summary)::float / word_count 
                         END) as compression_ratio
                FROM semantic_chunks
                GROUP BY level
                ORDER BY 
                    CASE level 
                        WHEN 'chunk' THEN 1 
                        WHEN 'message' THEN 2 
                        WHEN 'section' THEN 3 
                        WHEN 'conversation' THEN 4 
                    END
            """)
            
            stats = cursor.fetchall()
            
            print(f"{'Level':<12} {'Coverage':<10} {'Avg Summary':<12} {'Avg Content':<12} {'Compression':<12}")
            print("-" * 70)
            
            for stat in stats:
                coverage = (stat['has_summary'] / stat['total_chunks'] * 100) if stat['total_chunks'] > 0 else 0
                avg_summary = stat['avg_summary_length'] or 0
                avg_content = stat['avg_content_length'] or 0
                compression = stat['compression_ratio'] or 0
                
                print(f"{stat['level']:<12} {coverage:>6.1f}%     {avg_summary:>8.0f} chars {avg_content:>8.0f} words  {compression:>8.3f}")
            
            # Show some examples of good summaries
            print(f"\nüìù Example High-Quality Summaries:")
            print("-" * 40)
            
            cursor.execute("""
                SELECT chunk_id, level, summary, word_count
                FROM semantic_chunks
                WHERE summary IS NOT NULL 
                  AND LENGTH(summary) > 50 
                  AND word_count > 100
                ORDER BY LENGTH(summary) DESC
                LIMIT 3
            """)
            
            examples = cursor.fetchall()
            
            for i, example in enumerate(examples, 1):
                print(f"\n{i}. {example['level'].title()} Level ({example['word_count']} words ‚Üí {len(example['summary'])} chars)")
                wrapped = textwrap.fill(example['summary'], width=70, initial_indent="   ", subsequent_indent="   ")
                print(wrapped)
    
    def interactive_browse(self):
        """Interactive browsing session"""
        print("\nüîç Interactive Semantic Chunks Browser")
        print("Commands: list, explore <conv_id>, search <term>, details <chunk_id>, quality, help, quit")
        
        while True:
            try:
                command = input("\n> ").strip()
                
                if command.lower() in ['quit', 'q', 'exit']:
                    break
                
                elif command == 'list':
                    self.list_conversations_with_chunks()
                
                elif command.startswith('explore '):
                    try:
                        conv_id = int(command.split()[1])
                        self.explore_conversation_chunks(conv_id)
                    except (IndexError, ValueError):
                        print("‚ùå Usage: explore <conversation_id>")
                
                elif command.startswith('search '):
                    search_term = ' '.join(command.split()[1:])
                    if search_term:
                        self.search_chunks_by_summary(search_term)
                    else:
                        print("‚ùå Usage: search <search_term>")
                
                elif command.startswith('details '):
                    try:
                        chunk_id = command.split()[1]
                        self.show_chunk_details(chunk_id)
                    except IndexError:
                        print("‚ùå Usage: details <chunk_id>")
                
                elif command == 'quality':
                    self.analyze_summary_quality()
                
                elif command == 'help':
                    print("\nüÜò Available Commands:")
                    print("  list                    - Show conversations with chunks")
                    print("  explore <conv_id>       - Explore all chunks for a conversation")
                    print("  search <term>           - Search chunks by summary content")
                    print("  details <chunk_id>      - Show detailed info for a chunk")
                    print("  quality                 - Analyze summary quality statistics")
                    print("  help                    - Show this help")
                    print("  quit                    - Exit browser")
                
                else:
                    print("‚ùå Unknown command. Type 'help' for available commands.")
            
            except KeyboardInterrupt:
                print("\nüëã Goodbye!")
                break
            except Exception as e:
                print(f"‚ùå Error: {e}")

def main():
    """CLI entry point"""
    parser = argparse.ArgumentParser(
        description="Semantic Chunks Browser - Explore hierarchical chunks and summaries",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Interactive browsing
  python semantic_chunks_browser.py browse
  
  # List conversations with most chunks
  python semantic_chunks_browser.py list --limit 20
  
  # Explore specific conversation
  python semantic_chunks_browser.py explore --conversation-id 12345
  
  # Search summaries
  python semantic_chunks_browser.py search --term "consciousness"
  
  # Analyze summary quality
  python semantic_chunks_browser.py quality
        """
    )
    
    subparsers = parser.add_subparsers(dest='command', help='Commands')
    
    # Browse command (interactive)
    browse_parser = subparsers.add_parser('browse', help='Interactive browsing')
    
    # List command
    list_parser = subparsers.add_parser('list', help='List conversations with chunks')
    list_parser.add_argument('--limit', type=int, default=20, help='Number of conversations to show')
    
    # Explore command
    explore_parser = subparsers.add_parser('explore', help='Explore conversation chunks')
    explore_parser.add_argument('--conversation-id', type=int, required=True, help='Conversation ID to explore')
    
    # Search command
    search_parser = subparsers.add_parser('search', help='Search chunk summaries')
    search_parser.add_argument('--term', required=True, help='Search term')
    search_parser.add_argument('--limit', type=int, default=20, help='Number of results')
    
    # Details command
    details_parser = subparsers.add_parser('details', help='Show chunk details')
    details_parser.add_argument('--chunk-id', required=True, help='Chunk ID to examine')
    
    # Quality command
    quality_parser = subparsers.add_parser('quality', help='Analyze summary quality')
    
    args = parser.parse_args()
    
    browser = SemanticChunksBrowser()
    
    if args.command == 'browse':
        browser.interactive_browse()
    elif args.command == 'list':
        browser.list_conversations_with_chunks(args.limit)
    elif args.command == 'explore':
        browser.explore_conversation_chunks(args.conversation_id)
    elif args.command == 'search':
        browser.search_chunks_by_summary(args.term, args.limit)
    elif args.command == 'details':
        browser.show_chunk_details(args.chunk_id)
    elif args.command == 'quality':
        browser.analyze_summary_quality()
    else:
        # Default to overview
        print("Use --help for available commands, or try 'browse' for interactive mode")

if __name__ == "__main__":
    main()